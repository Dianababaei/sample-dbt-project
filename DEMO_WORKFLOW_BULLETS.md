# ðŸš€ Artemis dbtâ†’Snowflake Demo â€“ Workflow Bullets

## Overview
- **Goal:** Prove Artemis can optimise real dbt SQL while keeping financial reports 100% identical
- **Use Case:** Bain Capital portfolio analytics pipeline
- **Timeline:** 3 phases across ~2-3 weeks
- **Status:** Phase 1 (Input Project Setup) â€“ IN PROGRESS

---

## ðŸŽ¯ The Workflow (Visual)

```
BASELINE                  ARTEMIS                    VALIDATION
--------                  -------                    ----------
dbt project         â†’    Optimise SQL        â†’   compare_reports.py
sub-optimal SQL         Snowflake dialect       âœ… Bit-identical output
       â†“                       â†“                        â†“
run.sh              Modified models            âœ“ Performance â†“
       â†“                       â†“                âœ“ Cost â†“
baseline_report.csv  optimised_report.csv      âœ“ Report = Same
       â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                    COMPARISON (must match)
```

---

## ðŸ“‹ Phase 1: Build Input Project (THIS WEEK)

### What We Have âœ…
- âœ… Real dbt project (33 SQL models, 3 pipelines)
- âœ… Documented inefficiencies in each model (marked as "ISSUES FOR ARTEMIS")
- âœ… Schema definitions (sources.yml)
- âœ… Some seed data files (benchmarks, portfolios, hierarchy, date dimension)

### What We Need to Create âŒ
1. **Missing Seed Files** (realistic sample data)
   - `sample_cashflows.csv` (200 rows) â€“ cashflow transactions
   - `sample_trades.csv` (300 rows) â€“ trade records
   - `sample_positions_daily.csv` (100 rows) â€“ daily holdings
   - `sample_market_prices.csv` (500 rows) â€“ security prices
   - `sample_brokers.csv` (10 rows) â€“ broker reference
   - `sample_benchmark_returns.csv` (500 rows) â€“ benchmark data
   - `sample_portfolio_benchmarks.csv` (10 rows) â€“ mapping
   - `sample_valuations.csv` (100 rows) â€“ NAV data

2. **Local Database Setup**
   - Update `profiles.yml` to use **DuckDB** (no server needed, file-based)
   - Alternative: SQLite if DuckDB issues

3. **Execution Script** (`run.sh`)
   - Install dbt + dependencies
   - Load seeds (`dbt seed`)
   - Build all models (`dbt run`)
   - Run tests (`dbt test`)
   - Export final report (`dbt run-operation export_report`)
   - Capture metrics (`capture_metrics.py`)

4. **Macro to Export Report** (`macros/export_report.sql`)
   - Query the final report table
   - Export to CSV: `output/baseline_report.csv`
   - This becomes the **golden reference**

5. **Validation Script** (`scripts/compare_reports.py`)
   - Takes 2 CSV files (baseline vs optimised)
   - Checks:
     - âœ… Same row count
     - âœ… Same columns
     - âœ… **Identical values** (no rounding drift)
   - Returns: PASS/FAIL

6. **Documentation**
   - `SETUP_AND_RUN.md` â€“ How to execute locally
   - `DEMO_WORKFLOW_TEAM.md` â€“ Full detailed workflow (already created)

---

## ðŸ“‹ Phase 2: Hand Off to Artemis

### What Artemis Receives
- Complete dbt project with all models
- Sample data (seeds)
- `run.sh` script
- `baseline_report.csv` â€“ golden reference
- `baseline_metrics.json` â€“ performance baseline
- Brief: "Optimise SQL, keep report identical"

### Artemis Does This
- Resolves full dbt DAG (model lineage)
- Identifies inefficiencies (marked in comments)
- Rewrites SQL to Snowflake best practices:
  - Remove unnecessary DISTINCT
  - Push filters upstream
  - Collapse nested CTEs
  - Optimise window functions
  - Simplify joins
- Ensures all dbt tests still pass
- Returns: Optimised `models/` directory + new report

### Constraints (Non-negotiable)
- âŒ DO NOT change column names
- âŒ DO NOT change report structure
- âŒ DO NOT change numeric values
- âœ… DO optimise SQL syntax
- âœ… DO rewrite for Snowflake
- âœ… DO simplify inefficient patterns

---

## ðŸ“‹ Phase 3: Validation & Measurement

### Run Comparison
```bash
python scripts/compare_reports.py \
    output/baseline_report.csv \
    output/optimised_report.csv
```

### Success Criteria
- âœ… Row counts identical
- âœ… Column names identical
- âœ… All numeric values identical (bit-for-bit)
- âœ… All dbt tests pass (100%)
- âœ… No errors during execution

### Metrics to Capture
- dbt run time: **Before** vs **After** (e.g., 12.5s â†’ 8.2s = **1.52x faster**)
- Row counts per model: Must match
- Financial aggregates: Sum, avg, count (must match)
- Test results: All pass
- Database size: Optional (for cost estimation)

---

## ðŸŽ¯ Deliverables by Phase

### Phase 1 Deliverables
- [ ] Complete seed files (all 8 CSVs)
- [ ] `profiles.yml` configured for DuckDB
- [ ] `run.sh` script (fully working)
- [ ] `macros/export_report.sql` (exports final report)
- [ ] `scripts/compare_reports.py` (validation script)
- [ ] `scripts/capture_metrics.py` (metrics capture)
- [ ] `output/baseline_report.csv` (golden reference)
- [ ] `output/baseline_metrics.json` (baseline performance)
- [ ] `SETUP_AND_RUN.md` (instructions)

### Phase 2 Deliverables (from Artemis)
- [ ] Optimised `models/` directory
- [ ] `output/optimised_report.csv`
- [ ] Optimisation notes (what changed in each model)

### Phase 3 Deliverables
- [ ] Comparison report (baseline vs optimised)
- [ ] Validation results (PASS/FAIL)
- [ ] Metrics comparison (performance, costs)
- [ ] Demo-ready summary slide

---

## ðŸ—ï¸ Project Structure (End State)

```
sample-dbt-project/
â”‚
â”œâ”€â”€ models/                          # dbt models (will be optimised)
â”‚   â”œâ”€â”€ pipeline_a/
â”‚   â”‚   â”œâ”€â”€ staging/
â”‚   â”‚   â””â”€â”€ marts/
â”‚   â”œâ”€â”€ pipeline_b/
â”‚   â”œâ”€â”€ pipeline_c/
â”‚   â””â”€â”€ schema.yml
â”‚
â”œâ”€â”€ seeds/                           # Sample data CSVs
â”‚   â”œâ”€â”€ dim_date.csv âœ…
â”‚   â”œâ”€â”€ sample_benchmarks.csv âœ…
â”‚   â”œâ”€â”€ sample_cashflows.csv âŒ (TODO)
â”‚   â”œâ”€â”€ sample_trades.csv âŒ (TODO)
â”‚   â”œâ”€â”€ sample_positions_daily.csv âŒ (TODO)
â”‚   â”œâ”€â”€ sample_market_prices.csv âŒ (TODO)
â”‚   â”œâ”€â”€ sample_brokers.csv âŒ (TODO)
â”‚   â”œâ”€â”€ sample_benchmark_returns.csv âŒ (TODO)
â”‚   â”œâ”€â”€ sample_portfolio_benchmarks.csv âŒ (TODO)
â”‚   â””â”€â”€ sample_valuations.csv âŒ (TODO)
â”‚
â”œâ”€â”€ macros/
â”‚   â””â”€â”€ export_report.sql âŒ (TODO)
â”‚
â”œâ”€â”€ scripts/
â”‚   â”œâ”€â”€ compare_reports.py âŒ (TODO)
â”‚   â””â”€â”€ capture_metrics.py âŒ (TODO)
â”‚
â”œâ”€â”€ output/
â”‚   â”œâ”€â”€ baseline_report.csv âŒ (TODO â€“ will be generated)
â”‚   â””â”€â”€ baseline_metrics.json âŒ (TODO â€“ will be generated)
â”‚
â”œâ”€â”€ run.sh âŒ (TODO)
â”œâ”€â”€ profiles.yml (update needed)
â”œâ”€â”€ dbt_project.yml âœ…
â”‚
â”œâ”€â”€ DEMO_WORKFLOW_TEAM.md âœ… (detailed guide)
â”œâ”€â”€ DEMO_WORKFLOW_BULLETS.md âœ… (this file)
â””â”€â”€ SETUP_AND_RUN.md âŒ (TODO)
```

---

## âš¡ Quick Checklist (Copy-Paste for Jira/Notion)

### Phase 1 Tasks
- [ ] Create 8 seed CSV files with realistic data
- [ ] Update `profiles.yml` for DuckDB
- [ ] Write `run.sh` execution script
- [ ] Create `macros/export_report.sql`
- [ ] Write `scripts/compare_reports.py` validation
- [ ] Write `scripts/capture_metrics.py` metrics capture
- [ ] Execute `bash run.sh` locally
- [ ] Verify `output/baseline_report.csv` exists
- [ ] Verify `output/baseline_metrics.json` exists
- [ ] Document in `SETUP_AND_RUN.md`

### Phase 2 Tasks
- [ ] Package project for Artemis (zip or git push)
- [ ] Send brief: "Optimise SQL, keep report identical"
- [ ] Artemis returns optimised models
- [ ] Artemis returns `output/optimised_report.csv`

### Phase 3 Tasks
- [ ] Run: `python compare_reports.py baseline_report.csv optimised_report.csv`
- [ ] Verify: Reports are identical âœ…
- [ ] Capture metrics comparison
- [ ] Write demo summary

---

## ðŸ”‘ Key Rules (Golden Rules)

1. **Report is Sacred:** Even 1 changed digit = FAIL
2. **SQL is Flexible:** Rewrite, refactor, optimize freely
3. **Tests are Required:** All dbt tests must pass both before & after
4. **Validation is Automated:** compare_reports.py must return PASS
5. **No Snowflake Yet:** Use DuckDB for demo (easy to run locally)

---

## ðŸ“ž Roles & Responsibilities

| Role | Responsibility |
|------|-----------------|
| **Claude + Dev Team** | Build Phase 1 (seeds, scripts, baseline) |
| **Artemis Team** | Phase 2 (optimise SQL models) |
| **QA/Validation** | Phase 3 (run comparison, measure improvements) |
| **Product/Sales** | Phase 4 (demo to Bain Capital) |

---

## ðŸŽ¬ Demo Narrative (Once Complete)

> "We took a real Bain Capital dbt project with known SQL inefficiencies.
>
> Artemis analysed the full pipeline, rewrote the SQL for Snowflake best practices, and optimised joins, aggregations, and CTEs.
>
> The result? **Identical financial reports** (bit-for-bit), but:
> - âœ… SQL is faster (1.52x speedup)
> - âœ… Snowflake credits are lower
> - âœ… Code is cleaner and easier to maintain
>
> This is how Artemis keeps your data pipeline safe while making it faster and cheaper."

---

## ðŸ“Š Expected Results

| Metric | Before | After | Improvement |
|--------|--------|-------|-------------|
| dbt runtime | 12.5s | 8.2s | 1.52x faster |
| Snowflake credits | (estimate) 5.2 | (estimate) 3.1 | 40% reduction |
| Report rows | 420 | 420 | âœ… Identical |
| Report columns | 15 | 15 | âœ… Identical |
| Report values | Sum=$1.5B | Sum=$1.5B | âœ… Identical |
| dbt tests | 18/18 âœ… | 18/18 âœ… | âœ… All pass |

---

**Status:** Phase 1 â€“ IN PROGRESS
**Last Updated:** 2026-01-24
**Owner:** Claude + Team
