# Benchmarking Guide - Artemis dbt Optimization

**Quick Start for comparing baseline vs optimized pipelines**

---

## What You Get

âœ… dbt sample project (14 models, 65 tests, 49-row output)
âœ… Baseline report (original pipeline metrics)
âœ… Comparison framework (5-KPI model, all automatic)
âœ… Immediate cost estimation (bytes scanned â†’ credits, no 15-45 min delays)

---

## The 3 Steps

### 1ï¸âƒ£ Baseline (Original Code)

Already done. Located at:
```
benchmark/baseline/report.json
```

Contains:
- Runtime: 4.52 seconds
- Output: 49 rows, hash = `b006f42b...`

---

### 2ï¸âƒ£ Optimize

Artemis modifies SQL files in:
```
models/pipeline_a/marts/fact_cashflow_summary.sql
models/pipeline_a/staging/stg_cashflows.sql
```

---

### 3ï¸âƒ£ Compare

Run:
```bash
bash run_pipeline.sh
python benchmark/compare_kpis.py
```

**Output:**
```
KPI 1: EXECUTION TIME
  Baseline:  4.5464s
  Optimized: 2.8100s
  Change:    â†“ 38.1%

KPI 2: WORK METRICS
  Baseline:  104857600 bytes (100 MB)
  Optimized: 52428800 bytes (50 MB)
  Change:    â†“ 50.0%

KPI 3: OUTPUT VALIDATION
  Status: âœ… IDENTICAL (guaranteed no data drift)

KPI 4: QUERY COMPLEXITY
  Baseline:  7.5/10 (4 joins, 2 CTEs)
  Optimized: 5.2/10 (2 joins, 1 CTE)
  Change:    â†“ 30.7% simpler

KPI 5: COST ESTIMATION
  Baseline:  0.0001 credits (100 MB scanned)
  Optimized: 0.00005 credits (50 MB scanned)
  Change:    â†“ 50.0% fewer credits
```

---

## The 5 KPIs Explained (All Automatic)

| # | KPI | What | Why | Pass Condition |
|---|-----|------|-----|---|
| 1 | **Runtime** | Query execution time (s) | Faster = cheaper | Should â†“ |
| 2 | **Work Metrics** | Bytes scanned from QUERY_PROFILE | Direct cost proxy | Should â†“ or = |
| 3 | **Output Hash** | SHA256 of result set | Guarantee no drift | Must be **identical** |
| 4 | **Complexity** | Query structure analysis (joins, CTEs, window functions) | Automatic scoring 1-10 | Should â†“ or = |
| 5 | **Cost Estimation** | Credits estimated from bytes scanned (1 credit = 1 TB) | Reliable cost proxy, no waiting | Should â†“ or = |

---

## Interpreting Results

### âœ… Good Optimization
- Runtime â†“ 30-50%
- Rows = same
- Hash âœ… identical
- **Decision: ACCEPT**

### âŒ Invalid Optimization
- Hash âŒ different
- Numbers changed
- **Decision: REJECT** (breaks financial reporting)

### âš ï¸ Suspicious Optimization
- Runtime â†“ but hash changed
- **Decision: INVESTIGATE** (did logic change intentionally?)

---

## File Structure

```
benchmark/
â”œâ”€â”€ baseline/
â”‚   â””â”€â”€ report.json          â† Original metrics (golden truth)
â”œâ”€â”€ candidate/
â”‚   â””â”€â”€ report.json          â† Current metrics (generated each run)
â”œâ”€â”€ compare_kpis.py          â† Comparison script
â”œâ”€â”€ KPI_BENCHMARKING.md      â† Detailed KPI docs
â””â”€â”€ README.md                â† Generic benchmark framework
```

---

## One Command

```bash
bash run_pipeline.sh && python benchmark/compare_kpis.py
```

---

## Why This Framework Works

Snowflake's `ACCOUNT_USAGE.QUERY_HISTORY` has 15-45 min delay (system limitation), making credit-based benchmarking non-viable for rapid iteration.

Instead, we use **immediate, deterministic metrics**:
- **Runtime** - Wall-clock execution time (immediate)
- **Work Metrics** - Bytes scanned from QUERY_PROFILE (immediate, available right after query)
- **Output Hash** - SHA256 of result set (immediate, cryptographic guarantee)
- **Complexity Score** - Automatic query structure analysis (immediate)
- **Cost Estimation** - Bytes scanned â†’ credits calculation (immediate, no delays)

All KPIs are:
- âœ… Automatic (no manual inspection)
- âœ… Deterministic (same result every time)
- âœ… Immediate (available the moment the query finishes)
- âœ… Reliable (bytes scanned directly correlates with Snowflake cost)
- âœ… No external dependencies (don't rely on delayed billing systems)

---

## Next Steps

1. âœ… Original pipeline ready
2. ğŸ”„ Artemis optimizes SQL
3. âœ… Run `bash run_pipeline.sh`
4. âœ… Compare with `python benchmark/compare_kpis.py`
5. ğŸ“Š Show results to Bain Capital

---

## For Technical Details

See: `benchmark/KPI_BENCHMARKING.md`
